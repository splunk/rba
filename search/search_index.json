{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RBA all day","text":""},{"location":"#welcome-to-the-wonderful-world-of-risk-based-alerting","title":"Welcome to the wonderful world of Risk-Based Alerting!","text":"<p>RBA is Splunk's method to aggregate low-fidelity security events as interesting observations tagged with security metadata to create high-fidelity, low-volume alerts.</p>"},{"location":"#searches","title":"Searches","text":"<p>Useful SPL from the RBA community for working with risk events.</p>"},{"location":"#dashboards","title":"Dashboards","text":"<p>Simple XML or JSON for Splunk dashboards to streamline risk analysis.</p>"},{"location":"#risk-rules","title":"Risk Rules","text":"<p>Splunk's Threat Research Team has an incredible library of over 1000 detections in the Splunk's Enterprise Security Content Updates library. You can use Marcus Ferrera and Drew Church's awesome ATT&amp;CK Detections Collector to pop out a handy HTML file of relevant ESCU detections for you to align with MITRE ATT&amp;CK.</p>"},{"location":"#the-rba-community","title":"The RBA Community","text":"The RBA Community<pre><code>    Join the RBA Community Today!\n</code></pre> <p>The RBA Community is a group of professionals dedicated to advancing the field of risk-based alerting (RBA) and Splunk Enterprise Security (ES). Our mission is to provide a forum for sharing knowledge, best practices, and the latest developments in RBA and ES, and to help professionals enhance their understanding and skills in these areas.</p> <p>Whether you\u2019re new to RBA and ES or a seasoned pro, The RBA Community has something for everyone. We invite you to join us on this journey to enhance your understanding and expertise in RBA and ES \u2013 don\u2019t miss out on this opportunity to learn from the best and connect with other professionals in the field.</p> <p>Learn more </p>"},{"location":"#contributing","title":"Contributing","text":"<p>Want to contribute? See our contributing guidelines.</p>"},{"location":"#discussionfaq","title":"Discussion/FAQ","text":"<p>See discussions and frequently asked questions on our GitHub Discussions board.</p> <p>Visit Discussion Board </p>"},{"location":"contributing/contributing-guidelines/","title":"Contributing Guidelines","text":"<p>All are welcome to contribute!</p> <p>A GitHub account is required to create a pull request to submit new content. If you do not want to submit changes, you may also consider the following:</p> <ul> <li>Submit a feature request (issue) at https://github.com/splunk/rba/issues.</li> <li>Create a new discussion at https://github.com/splunk/rba/discussions.</li> <li>Don't have a GitHub account? Reach out to us on Slack!</li> </ul>"},{"location":"contributing/contributing-guidelines/#how-to-contribute","title":"How to Contribute","text":"<p>This repository uses MkDocs with the Material for MkDocs theme.</p> <p>If you know the markdown language then using this style of documentation will be a breeze. For a full list of capabilities see MkDocs's website.</p>"},{"location":"contributing/contributing-guidelines/#fork-the-rba-github","title":"Fork the RBA GitHub","text":"<ul> <li>Fork the RBA GitHub page, make your changes, and submit a pull requests.</li> <li>Try to match the format of the existing documentation. <ul> <li>We can assist you with this process. </li> </ul> </li> </ul>"},{"location":"contributing/contributing-guidelines/#create-a-local-environment-for-testing","title":"Create a local environment for testing","text":"<p>Testing locally will be a great way to ensure your changes will work with what currently exists. </p> <p>The easiest way to get started is by using a python virtual environment. For simplicity, <code>pipenv</code> will be used for the following.</p> <ol> <li>Install python and pipenv on your local workstation -&gt; Pipenv docs.</li> <li> <p>Once installed, navigate to your forked repository and run the following to install the latest requirements.</p> <pre><code># your forked rba directory\n# ./rba\npipenv install -r docs/requirements.txt\n</code></pre> </li> <li> <p>Now you can enter <code>pipenv run mkdocs serve</code> which will create a webserver that can be reached by opening your browser and navigating to http://localhost:8000.</p> </li> </ol>"},{"location":"contributing/contributors/","title":"Thanks to our GitHub Contributors!","text":"<p> <code>7thdrxn</code> - Haylee Mills</p> <p> <code>ZachChristensen28</code></p> <p> <code>matt-snyder-stuff</code></p> <p> <code>ZachTheSplunker</code></p> <p> <code>nterl0k</code> - Steven Dick</p> <p> <code>hettervik</code></p> <p> <code>RedTigR</code></p> <p> <code>Dean Luxton</code></p> <p> <code>elusive-mesmer</code></p> <p> <code>gabs - Gabriel Vasseur</code></p>"},{"location":"dashboards/","title":"Dashboards","text":""},{"location":"dashboards/#attck-matrix-risk-business-view","title":"ATT&amp;CK Matrix Risk (Business View)","text":"<p> attack_matrix_risk.xml</p> <p>Portrays risk in your environment through the lense of RBA and the MTRE ATT&amp;CK framework.</p>"},{"location":"dashboards/#attribution-analytics-tuning-view","title":"Attribution Analytics (Tuning View)","text":"<p> audit_attribution_analytics.xml</p> <p>Helpful for tuning new detections.</p>"},{"location":"dashboards/#rba-data-source-review","title":"RBA Data Source Review","text":"<p> rba_data_source_overview.xml</p> <p>This helps you to better what data sources you are using in RBA and see gaps in your coverage.</p>"},{"location":"dashboards/#risk-attributions-investigative-view","title":"Risk Attributions (Investigative View)","text":"<p> risk_attributions.xml</p> <p>Risk Attributions.</p>"},{"location":"dashboards/#risk-investigation","title":"Risk Investigation","text":"<p> risk_investigation.xml</p> <p>Risk Investigations.</p>"},{"location":"dashboards/#risk-notable-analysis","title":"Risk Notable Analysis","text":"<p> risk_notable_analysis_dashboard.xml</p> <p>Risk Notable Analysis.</p>"},{"location":"dashboards/attack_matrix_risk/","title":"ATT&amp;CK Matrix Risk (Business View)","text":"<p>View on GitHub </p> <p></p>"},{"location":"dashboards/audit_attribution_analytics/","title":"Attribution Analytics (Tuning View)","text":"<p>Helpful for tuning new detections.</p> <p>View on GitHub </p> <p></p>"},{"location":"dashboards/rba_data_source_overview/","title":"RBA Data Source Review","text":"<p>This helps you to better what data sources you are using in RBA and see gaps in your coverage.</p> <p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_attributions/","title":"Risk Attributions (Investigative View)","text":"<p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_investigation/","title":"Risk Investigation","text":"<p>View on GitHub </p> <p></p>"},{"location":"dashboards/risk_notable_analysis_dashboard/","title":"Risk Notable Analysis","text":"<p>View on GitHub </p> <p></p>"},{"location":"searches/","title":"Helpful Searches","text":"<p>These are some SPL techniques to get the most out of RBA by adding new features to your implementation or handling a common issue.</p>"},{"location":"searches/#integrate-ai-with-rir","title":"Integrate A&amp;I with RiR","text":"<p>Adding this SPL into your Risk Incident Rules normalizes your risk object to a unique key in the Asset &amp; Identity Framework; the primary advantage of this is throttling to prevent a Risk Incident Rule from firing on both a system and user that represent the same risk events.</p>"},{"location":"searches/#deduplicate-notables","title":"Deduplicate Notables","text":"<p>This feature will drastically reduce the number of duplicate Risk Notables by removing alerts where events are basically the same, already reviewed, or another Risk Incident Rule has already fired for.</p>"},{"location":"searches/#limit-score-stacking","title":"Limit score stacking","text":"<p>This SPL for your Risk Score Risk Incident Rules ensures that a single correlation search can only contribute risk a total of three times (or whatever you would like). This is handy for reducing rapidly stacking risk which is common early in the RBA maturation process.</p>"},{"location":"searches/#essential-rba-searches","title":"Essential RBA searches","text":"<p>This is all of the handy SPL contained in the Essential Guide to Risk Based Alerting; includes searches for finding noise, reducing noisy notables, and tuning risk rules.</p>"},{"location":"searches/#risk-info-field","title":"Risk info field","text":"<p>This is one of my favorite additions to RBA; adding this macro to your risk rules creates a field called risk_info (which you can add to your Risk Datamodel) containing all of the useful fields your analyst might use for analysis. It's in JSON formatting which allows easy manipulation in SPL and excellent material for dashboards and unique drilldowns per field.</p> <p>ADDITIONALLY, this frees risk_message to be used as a short and sweet summary rather than where you store all of the event detail. This lets Risk Notables tell a high level overview of events via risk_message, and is also handy to throttle or deduplicate by.</p>"},{"location":"searches/#chaining-behaviors","title":"Chaining behaviors","text":"<p>This is some simple SPL to organize risk events by risk_object and create risk rules which look for a specific sequence of events or chain of behaviors.</p>"},{"location":"searches/#naming-systemunknowncomputer-accounts","title":"Naming SYSTEM/Unknown/Computer Accounts","text":"<p>Computer accounts are used by Active Directory to authenticate machines to the domain, and RBA detections may find behavior in a log where the user account is simply listed as \"SYSTEM\" or even left blank because it is the computer account. This method renames the account to distinguish it as host$ from the noise of \"SYSTEM\" or \"unknown\". It can also be tied into the Asset &amp; Identify framework and contribute to detections on user risk objects.</p>"},{"location":"searches/asset_and_identity_rir_logic/","title":"Integrate Asset &amp; Identity Information into Risk Incident Rules","text":"<p>Note</p> <p>This feature has been added to ES 7.1, utilizing the <code>normalized_risk_object</code> field. This is also utilized for throttling in the default Risk Incident Rules which prevents notables from firing regularly on the same identity with different users or same asset with different hosts if A&amp;I is configured.</p> <p>This was a comment on this excellent Splunk Idea to <code>lower()</code> or <code>upper()</code> the risk_object in Risk Incident Rules, which goes one step further by integrating A&amp;I information:</p> <pre><code>| tstats `summariesonly` min(_time) as firstTime max(_time) as lastTime sum(All_Risk.calculated_risk_score) as risk_score, count(All_Risk.calculated_risk_score) as risk_event_count,values(All_Risk.annotations.mitre_attack.mitre_tactic_id) as annotations.mitre_attack.mitre_tactic_id, values(All_Risk.annotations.mitre_attack.mitre_technique_id) as annotations.mitre_attack.mitre_technique_id, values(All_Risk.tag) as tag, values(source) as source from datamodel=Risk.All_Risk by All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| eval risk_object=upper(risk_object)\n| lookup update=true identity_lookup_expanded identity as risk_object OUTPUTNEW _key as asset_identity_id,identity as asset_identity_value\n| lookup update=true asset_lookup_by_str asset as risk_object OUTPUTNEW _key as asset_identity_id,asset as asset_identity_value\n| eval asset_identity_risk_object=CASE(isnull(asset_identity_id),risk_object,true(),asset_identity_id)\n| stats min(firstTime) as firstTime max(lastTime) as lastTime sum(risk_score) as risk_score, sum(risk_event_count) as risk_event_count,values(annotations.mitre_attack.mitre_tactic_id) as annotations.mitre_attack.mitre_tactic_id, dc(annotations.mitre_attack.mitre_tactic_id) as mitre_tactic_id_count, values(annotations.mitre_attack.mitre_technique_id) as annotations.mitre_attack.mitre_technique_id, dc(annotations.mitre_attack.mitre_technique_id) as mitre_technique_id_count, values(tag) as tag, values(source) as source, dc(source) as source_count values(asset_identity_value) as asset_identity_value values(risk_object) as risk_object dc(risk_object) as risk_object_count by asset_identity_risk_object,risk_object_type\n| eval \"annotations.mitre_attack\"='annotations.mitre_attack.mitre_technique_id', risk_threshold=100\n| eval user=case(risk_object_type=\"user\",risk_object,true(),user),src=case(risk_object_type=\"system\",risk_object,true(),src)\n| where risk_score &gt;= $risk_threshold$\n| `get_risk_severity(risk_score)`\n| `security_content_ctime(firstTime)`\n| `security_content_ctime(lastTime)`\n</code></pre> <p>Note</p> <p>As they mention in the comment -- the one \"catch\" is you'll need to change your throttle object from \"risk_object\" to \"asset_identity_risk_object\" -- but this is great for preventing duplicate notables on the same basic user / system combination.</p>"},{"location":"searches/asset_and_identity_rir_logic/#extra-credit","title":"Extra Credit","text":"<p>Adding the above logic will increase the accuracy of Risk based alerting, however pivoting via the built in drilldown will still be limited. The following changes will allow analysts to pivot directly to all Risk alerts detected by the assoicated RIR.</p> <p>Create a macro called <code>get_risk_asset_ident(2)</code> with the following.</p> Macro Definition<pre><code>eval risk_in=\"$risk_object_in$\",risk_type_in=\"$risk_object_type_in$\"\n| lookup update=true identity_lookup_expanded identity as risk_object OUTPUTNEW _key as assetid_ident_id,identity as assetid_ident_value | lookup update=true asset_lookup_by_str asset as risk_object OUTPUTNEW _key as assetid_asset_id,asset as assetid_asset_value | lookup update=true identity_lookup_expanded identity as risk_in OUTPUTNEW _key as assetid_in_ident,identity as assetid_in_ident_value | lookup update=true asset_lookup_by_str asset as risk_in OUTPUTNEW _key as assetid_in_asset,asset as assetid_in_asset_value | eval risk_object_out=CASE((risk_type_in=\"user\" AND assetid_ident_id = 'assetid_in_ident'),assetid_in_ident_value, (risk_type_in=\"system\" AND (assetid_asset_id = 'assetid_in_asset')),assetid_in_asset_value)\n| eval risk_in=upper(risk_in)\n| eval risk_object=upper(risk_object)\n| where isnotnull(risk_object_out) OR (risk_object = risk_in)\n</code></pre> Arguments<pre><code>risk_object_in,risk_object_type_in\n</code></pre> <p> </p> get_risk_asset_ident(2) completed macro. <p>Update macro permissions</p> <p>Assign global scope (All apps) and allow all users read permission.</p>"},{"location":"searches/asset_and_identity_rir_logic/#update-existing-rir-drilldowns","title":"Update existing RiR drilldowns","text":"<p>Modify existing RIR drilldowns to include the macro similar to below.</p> Example<pre><code>| from datamodel:\"Risk.All_Risk\"  | `get_risk_asset_ident($risk_object|s$,$risk_object_type|s$)`\n| `get_correlations`  | rename annotations.mitre_attack.mitre_tactic_id as mitre_tactic_id, annotations.mitre_attack.mitre_tactic as mitre_tactic, annotations.mitre_attack.mitre_technique_id as mitre_technique_id, annotations.mitre_attack.mitre_technique as mitre_technique\n</code></pre> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/deduplicate_notables/","title":"Deduplicate Notable Events","text":"<p>Throttle Alerts Which Have Already Been Reviewed or Fired</p> <p>Because Risk Notables look at a period of time, it is common for a risk_object to keep creating notables as additional (and even duplicate) events roll in, as well as when events fall off as the time period moves forward. Additionally, different Risk Incident Rules could be firing on the same risk_object with the same events but create new Risk Notables. It is difficult to get around this with throttling, so here are some methods to deduplicate notables.</p>"},{"location":"searches/deduplicate_notables/#navigation","title":"Navigation","text":"<p>Here are two methods for Deduplicating Notable Events:</p> - Skill Level Pros Cons Method I Intermediate Deduplicates on front and back end More setup time Method II Beginner Easy to get started with Only deduplicates on back end"},{"location":"searches/deduplicate_notables/#method-i","title":"Method I","text":"<p>We'll use a Saved Search to store each Risk Notable's risk events and our analyst's status decision as cross-reference for new notables. Altogether new events will still fire, but repeated events from the same source will not. This also takes care of duplicate notables on the back end as events roll off of our search window.</p> <p>KEEP IN MIND</p> <p>Edits to the Incident Review - Main search may be replaced on updates to Enterprise Security; requiring you to make this minor edit again to regain this functionality. Ensure you have a step in your relevant process to check this search after an update.</p>"},{"location":"searches/deduplicate_notables/#1-create-a-truth-table","title":"1. Create a Truth Table","text":"<p>This method is described in Stuart McIntosh's 2019 .conf Talk (about 9m10s in), and we're going to create a similar lookup table. You can either download and import that file yourself, or create something like this in the Lookup Editor app:</p> <p> </p> Truth Table"},{"location":"searches/deduplicate_notables/#2-create-a-saved-search","title":"2. Create a Saved Search","text":"<p>Then we'll create a Saved Search which runs relatively frequently to store notable data and statuses.</p> <ol> <li>Navigate to Settings -&gt; Searches, reports, and alerts.</li> <li>Select \"New Report\" in the top right.</li> </ol> <p>Here is a sample to replicate</p> <p> </p> Sample Report With this SPL<pre><code>index=notable eventtype=risk_notables\n| eval indexer_guid=replace(_bkt,\".*~(.+)\",\"\\1\"),event_hash=md5(_time._raw),event_id=indexer_guid.\"@@\".index.\"@@\".event_hash\n| fields _time event_hash event_id risk_object risk_score source orig_source\n| eval temp_time=time()+86400\n| lookup update=true event_time_field=temp_time incident_review_lookup rule_id AS event_id OUTPUT status as new_status\n| lookup update=true correlationsearches_lookup _key as source OUTPUTNEW default_status\n| eval status=case(isnotnull(new_status),new_status,isnotnull(status),status,1==1,default_status)\n| fields - temp_time,new_status,default_status\n| eval temp_status=if(isnull(status),-1,status)\n| lookup update=true reviewstatuses_lookup _key as temp_status OUTPUT status,label as status_label\n| fields - temp_status\n| eval sources = if(isnull(sources) , orig_source , sources )\n| table _time event_hash risk_object source status_label sources risk_score\n| reverse\n| streamstats current=f window=0 latest(event_hash) as previous_event_hash values(*) as previous_* by risk_object\n| eval previousNotable=if(isnotnull(previous_event_hash) , \"T\" , \"F\" )\n| fillnull value=\"unknown\" previous_event_hash previous_status_label previous_sources previous_risk_score\n| eval matchScore = if( risk_score != previous_risk_score , \"F\" , \"T\" )\n| eval previousStatus = case( match(previous_status_label, \"(Closed)\") , \"nonmalicious\" , match(previous_status_label, \"(New|Resolved)\") , \"malicious\" , true() , \"malicious\" )\n# (1)!\n| mvexpand sources\n| eval matchRR = if(sources != previous_sources , \"F\", \"T\")\n| stats  dc(sources) as dcSources dc(matchRR) as sourceCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchRR = if(sourceCheckFlag &gt; 1 , \"F\" , matchRR )\n| lookup RIR-Truth-Table.csv previousNotable previousStatus matchRR matchScore OUTPUT alert\n| table _time risk_object source risk_score event_hash dcSources alert previousNotable previousStatus matchRR matchScore\n| outputlookup RIR-Deduplicate.csv\n</code></pre> <ol> <li><code>previousStatus</code> uses the default ES status label \"Closed\".</li> </ol> <p>In the SPL for <code>previousStatus</code> above, I used the default ES status label \"Closed\" as our only nonmalicious status. You'll have to make sure to use status labels which are relevant for your Incident Review settings. \"Malicious\" is used as the fallback status just in case, but you may want to differentiate \"New\" or unmatched statuses as something else for audit purposes; just make sure to create relevant matches in your truth table.</p> <p>I recommend copying the alert column from malicious events</p>"},{"location":"searches/deduplicate_notables/#schedule-the-saved-search","title":"Schedule the Saved Search","text":"Create schedule<pre><code>    Now find the search in this menu, click *Edit -&gt; Edit Schedule* and try these settings:\n</code></pre> <ul> <li>Schedule: Run on Cron Schedule</li> <li>Cron Expression: <code>*/3 * * * *</code></li> <li>Time Range: Last 7 days</li> <li>Schedule Priority: Highest</li> <li>Schedule Window: No window</li> </ul> <p>I made this search pretty lean, so running it every three minutes should work pretty well; I also decided to only look back seven days as this lookup could balloon in size and cause bundle replication issues. You probably want to stagger your Risk Incident Rule cron schedules by one minute more than this one so they don't fire on the same risk_object with the same risk events.</p>"},{"location":"searches/deduplicate_notables/#3-deduplicate-notables","title":"3. Deduplicate notables","text":"<p>Our last step is to ensure that the Incident Review panel doesn't show us notables when we've found a match to our truth table which doesn't make sense to alert on. In the Searches, reports, alerts page, find the search Incident Review - Main and click Edit -&gt; Edit Search.</p> <p>By default it looks like this:</p> <p> Default incident review search <p>And we're just inserting this line after the base search</p> Append to the base search<pre><code>...\n| lookup RIR-Deduplicate.csv _time risk_object source OUTPUTNEW alert\n| search NOT alert=\"no\"\n</code></pre> <p> </p> Updated incident review search"},{"location":"searches/deduplicate_notables/#congratulations","title":"Congratulations!","text":"<p>You should now have a significant reduction in duplicate notables</p> <p>If something isn't working, make sure that the Saved Search is correctly outputting a lookup (which should have Global permissions), and ensure if you <code>| inputlookup RIR-Deduplicate.csv</code> you see all of the fields being returned as expected. If Incident Review is not working, something is wrong with the lookup or your edit to that search.</p>"},{"location":"searches/deduplicate_notables/#extra-credit","title":"Extra Credit","text":"<p>If you utilize the Risk info field so you have a short and sweet risk_message, you can add another level of granularity to your truth table.</p> <p>if you utilize risk_message for ALL of the event detail, it may be too granular and isn't as helpful for throttling.</p> <p>This is especially useful if you are creating risk events from a data source with its own signatures like EDR, IDS, or DLP. Because the initial truth table only looks at score and correlation rule, if you have one correlation rule importing numerous signatures, you may want to alert when a new signature within that source fires.</p>"},{"location":"searches/deduplicate_notables/#create-a-calculated-field","title":"Create a calculated field","text":"<p>First, we'll create a new Calculated Field from risk_message in our Risk Datamodel called risk_hash with eval's <code>md5()</code> function, which bypasses the need to deal with special characters or other strangeness that might be in that field. If you haven't done this before - no worries - you just have to go to Settings -&gt; Data Models -&gt; Risk Data Model -&gt; Edit -&gt; Edit Acceleration and turn this off. Afterwards, you can Create New -&gt; Eval Expression like this:</p> <p> </p> Creating risk_hash from md5(risk_message) in data model Don't forget to re-enable the acceleration <p>You may have to rebuild the data model from the Settings -&gt; Data Model menu for this field to appear in your events.</p>"},{"location":"searches/deduplicate_notables/#update-spl","title":"Update SPL","text":"<p>Then we have to add this field into our Risk Incident Rules by adding this line to their initial SPL and ensure this field is retained downstream:</p> Field to add to RiR<pre><code>values(All_Risk.risk_hash) as risk_hashes\n</code></pre> <p>Now our Risk Notables will have a multi-value list of <code>risk_message</code> hashes. We must update our truth table to include a field called \"matchHashes\" - I've created a sample truth table here, but you must decide what is the proper risk appetite for your organization.</p> <p>Next we'll edit the Saved Search we created above to include the new fields and logic:</p> Updated logic (changes highlighted)<pre><code>...\n| eval sources = if(isnull(sources) , orig_source , sources )\n| table _time event_hash risk_object source status_label sources risk_score risk_hashes\n| reverse\n| streamstats current=f window=0 latest(event_hash) as previous_event_hash values(*) as previous_* by risk_object\n| eval previousNotable=if(isnotnull(previous_event_hash) , \"T\" , \"F\" )\n| fillnull value=\"unknown\" previous_event_hash previous_status_label previous_sources previous_risk_score previous_risk_hashes\n| eval matchScore = if( risk_score != previous_risk_score , \"F\" , \"T\" )\n| eval previousStatus = case( match(previous_status_label, \"(Closed)\") , \"nonmalicious\" , match(previous_status_label, \"(New|Resolved)\") , \"malicious\" , true() , \"malicious\" )\n| mvexpand risk_hashes\n| eval matchHashes= if(risk_hashes != previous_risk_hashes , \"F\" , \"T\" )\n| stats dc(matchHashes) as hashCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchHashes = if(hashCheckFlag &gt; 1 , \"F\" , matchHashes )\n| mvexpand sources\n| eval matchRR = if(sources != previous_sources , \"F\", \"T\")\n| stats  dc(sources) as dcSources dc(matchRR) as sourceCheckFlag values(*) as * by _time risk_object event_hash\n| eval matchRR = if(sourceCheckFlag &gt; 1 , \"F\" , matchRR )\n| lookup RIR-Truth-Table.csv previousNotable previousStatus matchRR matchScore matchHashes OUTPUT alert\n| table _time risk_object source risk_score event_hash dcSources alert previousNotable previousStatus matchRR matchScore matchHashes\n| outputlookup RIR-Deduplicate.csv\n</code></pre> <p>Voila! We now ensure that our signature-based risk rule data sources will properly alert if there are interesting new events for that risk object.</p>"},{"location":"searches/deduplicate_notables/#method-ii","title":"Method II","text":"<p>This method is elegantly simple to ensure notables don't re-fire as earlier events drop off the rolling search window of your Risk Incident Rules. It does this by only firing if the latest risk event is from the past 70 minutes.</p> Append to existing RIR<pre><code>...\n| stats latest(_indextime) AS latest_risk\n| where latest_risk &gt;= relative_time(now(),\"-70m@m\")\n</code></pre> <p>Credit to Josh Hrabar and Josh Campbell, this is brilliant. Thanks y'all!</p> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/limit_score_stacking/","title":"Limit Risk Rule Score Stacking","text":"<p>These will help reduce the maximum amount of risk which can be added from noisy Risk Rules.</p>"},{"location":"searches/limit_score_stacking/#navigation","title":"Navigation","text":"<p>There are two methods for limiting score stacking</p> - Skill Level Pros Cons Method I Beginner Easy to get started with Less context around what was capped and why Method II Intermediate More precise deduplication and additional information Additional understanding of SPL"},{"location":"searches/limit_score_stacking/#method-i","title":"Method I","text":"<p>This caps the risk score contribution of a single source by 3x the highest score from that source.</p> <pre><code>| tstats summariesonly=true sum(All_Risk.calculated_risk_score) as summed_risk_score max(All_Risk.calculated_risk_score) as single_risk_score dc(source) as source_count count\n FROM datamodel=Risk.All_Risk\n WHERE All_Risk.risk_object_type=\"*\" (All_Risk.risk_object=\"*\" OR risk_object=\"*\")\nBY All_Risk.risk_object All_Risk.risk_object_type source\n| eval capped_risk_score=if(summed_risk_score &lt; single_risk_score*3, summed_risk_score, single_risk_score*3)\n| stats sum(capped_risk_score) as capped_risk_score sum(summed_risk_score) as summed_risk_score dc(source) as source sum(count) as count\n BY All_Risk.risk_object All_Risk.risk_object_type\n| sort 1000 - risk_score\n...\n</code></pre> <p>Note</p> <p>You may want to limit this to particular sources, but this is extra handy for noisy sources like EDR, DLP, or IDS.</p> <p>Thanks David Dorsey!</p>"},{"location":"searches/limit_score_stacking/#method-ii","title":"Method II","text":"<p>This option adds some complexity, however, provides more information and better deduplication. The full write-up of how to accomplish this method can be found on gabs website.</p> <p>Visit Website </p> <p> *reference: https://www.gabrielvasseur.com/post/rba-a-better-way-to-dedup-risk-events</p> Final SPL from blog post<pre><code>| inputlookup TEMP_GABS_riskybusiness.csv\n``` First we take the breakdown of what actually happened, before doing any kind of deduping ```\n| eventstats sum(count) as count_msg\n    by risk_object risk_object_type risk_score source risk_message ```Get breakdown per risk_message``` \n| eventstats values(eval(count_msg.\"*\".risk_score)) as breakdown_msg\n    by risk_object risk_object_type            source risk_message ```Get breakdown per risk_message```\n| eventstats sum(count) as count_src\n    by risk_object risk_object_type risk_score source ```Get breakdown per source```\n| eventstats values(eval(count_src.\"*\".risk_score)) as breakdown_src\n    by risk_object risk_object_type            source ```Get breakdown per source```\n| stats sum(count) as risk_event_count, values(breakdown_src) as breakdown_src,\n    values(breakdown_msg) as breakdown_msg, sum(eval(risk_score*count)) as total_score,\n    max(risk_score) as max_score, latest(_time) as _time, values(mitre_*) as mitre_*\n    by risk_object risk_object_type source risk_message ```Reduce to unique risk_message\n    (it's not impossible to have several risks with the same risk_message but different scores)```\n| eval risk_message= mvjoin(breakdown_msg,\"+\").\"=\".max_score\n    . if( total_score!=max_score, \" (!\" . total_score . \")\", \"\") . \" \" .risk_message\n``` START limit to a maximum of 10 contributions per source ```\n| sort 0 risk_object risk_object_type source - max_score ``` Only the lowest scores will be dedup'd ```\n| eventstats dc(risk_message) as dc_msg_per_source by risk_object risk_object_type source \n| streamstats count as rank_per_source by risk_object risk_object_type source \n| eval risk_message=case( \n    rank_per_source &lt;= 10, risk_message,\n    rank_per_source = 11, \"...+\" . ( dc_msg_per_source - 20 ) . \" others from '\" . source . \"'...\" ,\n    1==1, null() ) \n| eval max_score=if( rank_per_source &lt;= 10, max_score, 0 )\n``` END limit to a maximum of 10 contributions per source ```\n| stats sum(risk_event_count) as risk_event_count, values(breakdown_src) as breakdown_src,\n    list(risk_message) as risk_message, sum(max_score) as risk_score,\n    sum(total_score) as risk_score_nodedup, latest(_time) as _time, values(mitre_*) as mitre_*\n    by risk_object risk_object_type source ```Reduce to unique source```\n| eval breakdown_src = mvjoin(breakdown_src,\"+\") .\"=\".risk_score\n    . if( risk_score!=risk_score_nodedup, \" (!\" . risk_score_nodedup . \")\", \"\" ) . \" \".source\n| stats sum(risk_event_count) as risk_event_count, list(source) as source, dc(source) as source_count,\n    list(breakdown_src) as srcs, list(risk_message) as risk_message, sum(risk_score) as risk_score,\n    sum(risk_score_nodedup) as risk_score_nodedup, latest(_time) as _time, values(mitre_*) as mitre_*,\n    dc( mitre_tactic_id) as mitre_tactic_id_count, dc(mitre_technique_id) as mitre_technique_id_count\n    by risk_object risk_object_type ```Reduce to unique object```\n</code></pre> <p>Authors</p> @7thdrxn - Haylee Mills @gabs - Gabriel Vasseur"},{"location":"searches/naming_system_unknown_computer_accounts/","title":"Naming SYSTEM / Unknown / Computer Accounts - The SEAL Method","text":"<p>Computer accounts are used by Active Directory to authenticate machines to the domain, and RBA detections may find behavior in a log where the user account is simply listed as \"SYSTEM\" or even left blank because it is the computer account. This method renames the account to distinguish it as host$ from the noise of \"SYSTEM\" or \"unknown\". It can also be tied into the Asset &amp; Identify framework and contribute to detections on user risk objects.</p>"},{"location":"searches/naming_system_unknown_computer_accounts/#steps","title":"Steps","text":"<p>Navigate to Settings &gt; Fields &gt; Calculated Fields &gt; Add New</p> Setting Value Source <code>XmlWinEventLog:Security</code> Source <code>XmlWinEventLog:Microsoft-Windows-Sysmon/Operational</code> Name <code>user</code> Eval Expression <code>if(user=\"SYSTEM\" OR user=\"-\",'host'+\"$\",'user')</code> Conflicting knowledge objects - Sysmon TA <p>We have to be careful with existing order of knowledge objects and calculated fields. The Sysmon TA already has a <code>user = \"\"</code> calculated field which we can update as follows:</p> Existing:<pre><code>user = upper(case(\nNOT isnull(User) AND NOT User IN (\"-\"), replace(User, \"(.*)\\\\\\(.+)$\",\"\\2\"),\n    NOT isnull(SourceUser) AND NOT isnull(TargetUser) AND SourceUser==TargetUser, replace(SourceUser, \"(.*)\\\\\\(.+)$\",\"\\2\")\n))\n</code></pre> Update to:<pre><code>user = upper(case(\nmatch(User,\".+\\\\\\SYSTEM\"), host.\"$\",\n    NOT isnull(User) AND NOT User IN (\"-\"), replace(User, \"(.*)\\\\\\(.+)$\",\"\\2\"),\n    NOT isnull(SourceUser) AND NOT isnull(TargetUser) AND SourceUser==TargetUser, replace(SourceUser, \"(.*)\\\\\\(.+)$\",\"\\2\")\n))\n</code></pre>"},{"location":"searches/naming_system_unknown_computer_accounts/#extra-credit","title":"Extra Credit","text":"<p>Not going to map this entire process due to how different it can be in each environment, but you can now add the computer account to your Identity lookup to aggregate with other user accounts. For example, you might take the fields <code>nt_host</code> and <code>owner</code> from your Asset lookup (asset_lookup_by_str), then map <code>owner</code> to <code>email</code> in the Identity lookup (identity_lookup_expanded). If you make a saved search that outputs a CSV, you can now use that to add fields into your Identity lookup.</p> <p>Authors</p> @Dean Luxton @StevenD"},{"location":"searches/risk_guide_searches/","title":"Essential RBA searches","text":"<p>Handy SPL contained in the Essential Guide to Risk Based Alerting.</p>"},{"location":"searches/risk_guide_searches/#determine-correlation-searches-with-high-falsebenign-positive-rates","title":"Determine Correlation Searches with High False/Benign Positive Rates","text":"<pre><code>`notable`\n| stats count(eval(status_label=\"Incident\")) as incident count(eval(status_label=\"Resolved\")) as closed\n BY source\n| eval benign_rate = 1 - incident / (incident + closed)\n| sort - benign_rate\n</code></pre> Note <p>Be sure to replace the <code>status_label</code> with whatever is used in your environment.</p>"},{"location":"searches/risk_guide_searches/#risk-rules-generating-the-most-risk","title":"Risk Rules Generating the Most Risk","text":"<pre><code>| tstats summariesonly=false sum(All_Risk.calculated_risk_score)\nas risk_score,dc(All_Risk.risk_object)\nas risk_objects,count\n FROM datamodel=Risk.All_Risk\n WHERE * All_Risk.risk_object_type=\"*\" (All_Risk.risk_object=\"*\" OR risk_object=\"*\")\nBY source\n| sort 1000 - count risk_score\n</code></pre>"},{"location":"searches/risk_guide_searches/#dig-into-noisy-threat-objects","title":"Dig into Noisy Threat Objects","text":"<pre><code>| tstats summariesonly=true count dc(All_Risk.risk_object) as dc_objects dc(All_Risk.src) as dc_src dc(All_Risk.dest) as dc_dest dc(All_Risk.user) as dc_users dc(All_Risk.user_bunit) as dc_bunit sum(All_Risk.calculated_risk_score) as risk_score values(source) as source\nFROM datamodel=Risk.All_Risk\n BY All_Risk.threat_object,All_Risk.threat_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| sort 1000 - risk_score\n</code></pre>"},{"location":"searches/risk_guide_searches/#find-noisiest-risk-rules-in-risk-notables","title":"Find Noisiest Risk Rules in Risk Notables","text":"<pre><code>index=notable eventtype=risk_notables\n| stats count\n BY orig_source\n| eventstats sum(count) as total\n| eval percentage = round((count / total) * 100,2)\n| sort - percentage\n</code></pre>"},{"location":"searches/risk_guide_searches/#structural-changes","title":"Structural Changes","text":""},{"location":"searches/risk_guide_searches/#notable-macro-to-edit-for-qa-risk-notables","title":"Notable Macro to Edit for QA Risk Notables","text":"<p>Add <code>| eval QA=1</code> to the end of your Risk Incident Rules, editing the macro <code>get_notable_index</code> from the default to begin \"QA\" mode.</p> default<pre><code>index=notable\n</code></pre> QA mode<pre><code>index=notable NOT QA=1\n</code></pre> <p>This will keep Risk Notables out of your Incident Review queue while you develop RBA.</p>"},{"location":"searches/risk_guide_searches/#create-a-sandbox-for-risk-rules-away-from-risk-notables","title":"Create a Sandbox for Risk Rules away from Risk Notables","text":"<p>Create an eventtype called something like <code>QA</code> and have it apply a tag called <code>QA</code>, then add the following to your Risk Incident Rules.</p> <pre><code>...\nWHERE NOT All_Risk.tag=QA\n...\n</code></pre> <p>This keeps your curated risk ecology preserved so you can compare how many Risk Notables you would see if your QA content was added.</p>"},{"location":"searches/risk_guide_searches/#include-previous-notables-in-new-notables","title":"Include Previous Notables in New Notables","text":"<p>If you create a lookup from a saved search called <code>Past7DayNotables.csv</code> where you store the previous time, status, and sources, you could include this in your Risk Incident Rules:</p> <pre><code>| lookup Past7DayNotables.csv risk_object OUTPUT prev_time prev_status prev_sources\n| eval prev_alerts = prev_time.\" - \".prev_status.\" - \".prev_sources\n</code></pre> Note <p>Make sure to add <code>prev_alerts</code> to the Incident Review Settings page so this shows up in the Incident Review panel.</p>"},{"location":"searches/risk_guide_searches/#tuning","title":"Tuning","text":""},{"location":"searches/risk_guide_searches/#remove-results-with-a-lookup","title":"Remove Results with a Lookup","text":"<p>Once you have a lookup built out, insert it into a search like this:</p> <pre><code>index=proxy http_method=\"POST\" NOT\n  [| inputlookup RR_Proxy_Allowlist.csv\n  | fields Web.src Web.dest\n  | rename Web.* AS *]\n</code></pre> <p>You could also do this with a datamodel:</p> <pre><code>| tstats summariesonly=t values(Web.dest) as dest from datamodel Web.Web where Web.http_method=\"POST\" NOT\n  [| inputlookup RR_Proxy_Allowlist.csv | fields Web.src Web.dest]\nby _time, Web.src\n</code></pre> <p>Using the Web datamodel field constraints as an example so we can properly exclude results from index or datamodel based risk rules.</p>"},{"location":"searches/risk_guide_searches/#adjust-risk-scores","title":"Adjust Risk Scores","text":"<ul> <li>Using eval</li> <li>Using lookup</li> </ul>"},{"location":"searches/risk_guide_searches/#using-eval","title":"Using <code>eval</code>","text":"<pre><code>index=proxy signature=*\n| table src user user_bunit dest signature http_code\n| eval risk_score = case(\nsignature=\"JS:Adware.Lnkr.A\",\"10\",\n  signature=\"Win32.Adware.YTDownloader\",\"0\",\n  NOT http_code=\"200\",\"25\",\n  signature=\"Trojan.Win32.Emotet\" AND NOT user_bunit=\"THREAT INTELLIGENCE\",\"100\"\n)\n</code></pre> <p>In this example, we are:</p> <ul> <li>Assigning the score of 10 for a signature that isn't generally bad but we still want to add a small amount of risk.</li> <li>Zeroing out the score for a signature of something a lot of our users have installed and we can't really control, but still want to observe is happening.</li> <li>Assigning the score of 25 for an unsuccessful HTTP connection.</li> <li>Assigning the score of 100 and potentially alerting directly in case we see malware from someone who is not on the Threat Intelligence team.</li> <li>Assigning a null() value in every other case to utilize the default risk score from the Risk Analysis action.</li> </ul>"},{"location":"searches/risk_guide_searches/#using-lookup","title":"Using <code>lookup</code>","text":"<pre><code>index=proxy signature=*\n| table src user user_bunit dest signature http_code\n| lookup RR_Proxy_Adjust.csv src user user_bunit dest signature http_code OUTPUTNEW risk_score\n</code></pre> <p>We can do the same with a lookup and as many relevant fields as we need for the most constrained exclusions.</p>"},{"location":"searches/risk_guide_searches/#dedup-similar-events-from-counting-multiple-times-in-risk-notables-score","title":"Dedup Similar Events from Counting Multiple Times in Risk Notables (Score)","text":"<pre><code>...\n BY All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| streamstats sum(risk_score) as original_score values(source) as sources values(risk_message) as risk_messages by risk_object\n| eval adjust_score = case(\nsource IN (\"My Noisy Rule That Fires a Lot but I Still Want to Know About, Once\", \"My Other Really Useful Context Low Risk Rule\"),\"1\",\n match(risk_message,\"IDS - Rule Category 1.*|IDS - Rule Category 2.*\") OR match(risk_message,\"DLP - Rule Category 1.*|DLP - Rule Category 2.*\"),\"1\",\n 1=1,null())\n| eval combine = coalesce(adjust_score,risk_message)\n| dedup combine risk_score\n| streamstats sum(risk_score) as risk_score values(sources) as source values(risk_messages) as risk_message by risk_object\n...\n</code></pre> <p>For making sure similar detections on basically the same event only count once in our total risk score.</p>"},{"location":"searches/risk_guide_searches/#weight-events-from-noisy-sources-in-risk-notables-metadata","title":"Weight Events from Noisy Sources in Risk Notables (Metadata)","text":"<pre><code>...\nBY All_Risk.risk_object,All_Risk.risk_object_type\n| `drop_dm_object_name(\"All_Risk\")`\n| mvexpand source\n| lookup RIRadjust-rule_weight.csv source OUTPUTNEW mitre_weight source_weight\n| eval mitre_weight = if(isnotnull(mitre_weight),mitre_weight,\"0\")\n| eval source_weight = if(isnotnull(source_weight),source_weight,\"0\")\n| streamstats sum(mitre_weight) as mitre_weight_total sum(source_weight) as source_weight_total values(*) as * by risk_object risk_object_type\n| eval mitre_tactic_id_count = mitre_tactic_id_count - mitre_weight_total\n| eval source_count = source_count - source_weight_total\n| eval \"annotations.mitre_attack\" = 'annotations.mitre_attack.mitre_technique_id'\n| where mitre_tactic_id_count &gt;= 3 and source_count &gt;= 4\n</code></pre> <p>For tuning Risk Incident Rules that don't rely on an accretive score to alert, but still need a lever to tweak noisy sources. In our example lookup, we would include a value between 0 and 1 for each noisy source; IE 0.75 to only count a rule as \u00bc of a standard weight, 0.5 to only count as \u00bd, etc.</p> <p>Authors</p> @7thdrxn - Haylee Mills"},{"location":"searches/risk_info_event_detail/","title":"Risk info field","text":""},{"location":"searches/risk_info_event_detail/#create-macro-for-risk_info-field","title":"Create macro for risk_info field","text":"<p>You may want to keep risk_message relatively brief as a sort of high-level overview of a risk event, then utilize a new field to store details. We can create a macro called <code>risk_info(1)</code> to create a JSON-formatted field with this SPL:</p> Macro definition<pre><code>eval risk_info = \"{\\\"risk_info\\\":{\"\n| foreach $fields$\n    [\n| eval &lt;&lt;FIELD&gt;&gt;=if(isnull(&lt;&lt;FIELD&gt;&gt;), \"unknown\", &lt;&lt;FIELD&gt;&gt;)\n    ```Preparing json array if FIELD is multivalue, otherwise simple json value```\n| eval json=if(mvcount(&lt;&lt;FIELD&gt;&gt;)&gt;1,mv_to_json_array(mvdedup(&lt;&lt;FIELD&gt;&gt;)),\"\\\"\".&lt;&lt;FIELD&gt;&gt;.\"\\\"\") \n    | eval risk_info=risk_info.\"\\\"\".\"&lt;&lt;FIELD&gt;&gt;\".\"\\\": \".json.\",\"\n    ]\n| rex mode=sed field=risk_info \"s/,$/}}/\"\n| fields - json\n</code></pre> <p>  Many thanks to RedTigR on the RBA Slack for providing the multi-value friendly version of this macro.</p> <p>Utilizing the macro like <code>risk_info(\"field1,field2,field3,etc\")</code> to give us a JSON formatted field with any of the fields we like.</p> <p>And then if we wanted to break this out in a dashboard we could use <code>spath</code> to break out fields into their own columns, or a rex command like this:</p> <p>Example</p> <pre><code>| rex field=risk_info max_match=100 \"(?&lt;risk_info2&gt;\\\"\\w+\\\":\\s*((?:(?&lt;!\\\\\\)\\\"[^\\\"]*\\\"|\\[[^\\]]*\\]))(?=,|\\s*}))\"\n</code></pre> <p>To break out each field as a multi-value on their own line in the same column. It looks really pretty, and you can even use <code>$click.value2$</code> to determine exactly which MV field was clicked and utilize different drilldowns per field, for example.</p>"},{"location":"searches/risk_info_event_detail/#extracting-existing-fields-from-risk-events-into-risk_info-field","title":"Extracting existing fields from risk events into risk_info field","text":"<p>Assumption</p> <p>Your risk rules are outputting specific details in addition to the risk fields (e.g. <code>risk_message</code>, <code>risk_object</code> etc.)</p> <p>The following search replaces the <code>View the individual Risk Attributions</code> drilldown within a risk incident rule. It allows us to dynamically bring the output of each individual risk rule in a concise manner.</p> <p>The aim of this is to minimize pivoting when performing the initial assessment of a risk incident while keeping the notable and <code>risk_message</code> field concise.</p> <pre><code>index=risk\n| search risk_object=$risk_object$\n| rename annotations.mitre_attack.mitre_tactic_id AS mitre_tactic_id, annotations.mitre_attack.mitre_tactic AS mitre_tactic\n| rex field=_raw max_match=0 \"(?&lt;risk_info&gt;[^\\=]+\\=\\\"([^\\\"]+\\\")+?)((, )|$)\"\n| eval risk_info=mvfilter(NOT match(risk_info, \"^(annotations)|(info_)|(savedsearch_description)|(risk_)|(orig_time)|(([0-9]+, )?search_name)\"))\n| table _time, source, risk_object, risk_score, risk_message, risk_info, risk_object_type, mitre_tactic_id, mitre_tactic\n| eval calculated_risk_score=risk_score\n| sort _time\n</code></pre> <p>Breaking down some decisions:</p> <ul> <li><code>| rex field=_raw</code> instead of <code>| foreach *</code> since Splunk adds in additional fields which aren't in the original risk rule output. This was made so the output is as concise and as relevant as possible. However, foreach is another method and it isn't reliant on regex.</li> <li><code>calculated_risk_score</code> is a required field for the drilldown so it displays properly in the Risk Events panel.</li> <li>If you are providing _time in your risk rules, you could rename <code>_time</code> to <code>observation_time</code> and <code>orig_time</code> to <code>_time</code> for a more accurate chronological order of events.</li> <li>The datamodel could be used, but if you wanted accelerated searching via <code>tstats</code> you would need to customize it in some way such as including the <code>_raw</code> field, which may be costly. Creating a risk_info field with the macro above would be more efficient.</li> </ul> <p>Authors</p> @7thdrxn - Haylee Mills @RedTigR @elusive-mesmer"},{"location":"searches/this_then_that_alerts/","title":"Detect Chain of Behaviors","text":"<p>To make a risk rule that looks for two rules firing close together, we can use <code>sort</code> followed by the <code>autoregress</code> command within a certain duration:</p> <pre><code>index=risk sourcetype=stash search_name=\"Search1\" OR search_name=\"Search2\"\n| sort by user _time | dedup _time search_name user\n| delta _time as gap\n| autoregress search_name as prev_search\n| autoregress user as prev_user\n| where user = prev_user\n| table _time gap src user prev_user search_name prev_search\n| where ((search_name=\"Search1\" OR search_name=\"Search2\") AND (prev_search=\"Search1\" OR prev_search=\"Search2\") AND gap&lt;600)\n</code></pre> <p>The benefit of not doing this in a single search is you still have the individual risk events as useful observations, and then can add more risk when observed together, or tweak risk down for noisy events without \"allowlisting\" altogether.</p> <p>Authors</p> @7thdrxn - Haylee Mills"}]}